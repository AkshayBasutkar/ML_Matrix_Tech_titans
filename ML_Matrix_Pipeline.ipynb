{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "t2H2OmO3-EtR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import spacy\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity as cs_tfidf\n",
        "from transformers import pipeline\n",
        "import tensorflow as tf\n",
        "from transformers import T5Tokenizer, TFT5ForConditionalGeneration\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "sRSn96kQS2kG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DILl2cDWQIX"
      },
      "outputs": [],
      "source": [
        "class Examiner:\n",
        "    def __init__(self):\n",
        "        self.model = TFT5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "        self.tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "        self.model.load_weights(\"/content/drive/MyDrive/ML_Matrix_hackhathon/Models/tf_model.h5\")\n",
        "\n",
        "    def preprocessing(self, text):\n",
        "        vocabulary=text\n",
        "        j=0\n",
        "        sentences=[]\n",
        "        vocabulary=vocabulary.replace('\\n',' ')\n",
        "        for i in range(len(vocabulary)):\n",
        "            if (vocabulary[i]=='.' or vocabulary[i]=='!' or vocabulary[i]=='?'):\n",
        "                sentences.append(vocabulary[j+1:i])\n",
        "                j=i\n",
        "\n",
        "        input=5  #input given by user for number of questions\n",
        "        if(input<=len(sentences)):\n",
        "            len(sentences)%input\n",
        "        else:\n",
        "            print('Enter No. of questions less than or equal to ',len(sentences))\n",
        "\n",
        "\n",
        "\n",
        "        dit={}\n",
        "        dit['ns']=len(sentences)\n",
        "        for i in range(len(sentences)):\n",
        "            dit[i]=sentences[i]\n",
        "\n",
        "        return dit\n",
        "\n",
        "    def summary(self):\n",
        "        import numpy as np\n",
        "        def summarize_text(text, top_k=3):\n",
        "        # Load BERT model and tokenizer\n",
        "        tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "        model = TFAutoModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        # Tokenization\n",
        "        sentences = sent_tokenize(text)\n",
        "\n",
        "        # Get embeddings for multiple sentences\n",
        "        embeddings = []\n",
        "        for sentence in sentences:\n",
        "            inputs = tokenizer(sentence, return_tensors='tf', truncation=True, padding=True)\n",
        "            outputs = model(inputs)\n",
        "            sentence_embedding = outputs.last_hidden_state[:, 0, :].numpy()\n",
        "            embeddings.append(sentence_embedding)\n",
        "\n",
        "        sentence_embeddings = np.vstack(embeddings)\n",
        "\n",
        "        # Ranking sentences based on similarity to the document embedding\n",
        "        doc_embedding = np.mean(sentence_embeddings, axis=0)\n",
        "        doc_embedding = normalize([doc_embedding])\n",
        "        sentence_embeddings = normalize(sentence_embeddings)\n",
        "        scores = cosine_similarity(doc_embedding, sentence_embeddings)\n",
        "\n",
        "        ranked_sentences = [(sentences[i], scores[0][i]) for i in range(len(sentences))]\n",
        "        ranked_sentences = sorted(ranked_sentences, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Picking the top_k sentences which are most relevant\n",
        "        top_sentences = [sent[0] for sent in ranked_sentences[:top_k]]\n",
        "        summary = ' '.join(top_sentences)\n",
        "\n",
        "        return summary\n",
        "\n",
        "    def generate_questions(self, input_set, max_length=64):\n",
        "        questions = []\n",
        "        diff = input_set['DL']\n",
        "        no_questions = input_set['NQ']\n",
        "        for i in range(no_questions):\n",
        "            current_context = f\"S{i}\"\n",
        "            current_context = input_set[current_context]\n",
        "\n",
        "            if diff == 1:\n",
        "                answer = \"\"\n",
        "            elif diff == 2:\n",
        "                answer = \"\"\n",
        "            elif diff == 3:\n",
        "                answer = \"\"\n",
        "\n",
        "            input_text = f\"generate question: {current_context} answer: {answer}\"\n",
        "            input_ids = self.tokenizer.encode(input_text, return_tensors=\"tf\")\n",
        "\n",
        "            outputs = self.model.generate(input_ids, max_length=max_length, num_return_sequences=1, no_repeat_ngram_size=2)\n",
        "            generated_question = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "            questions.append(generated_question)\n",
        "\n",
        "        return questions\n",
        "\n",
        "    def evaluate_answers(self):\n",
        "    def evaluate_answers(questions_dict, answers_dict):\n",
        "        # Load necessary models\n",
        "        nlp = spacy.load('en_core_web_sm')\n",
        "        emotion_classifier = pipeline('sentiment-analysis', model='j-hartmann/emotion-english-distilroberta-base')\n",
        "        sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "        lst = []\n",
        "        results = {}\n",
        "\n",
        "        def semantic_similarity(user_answer, correct_answer):\n",
        "            embeddings = sentence_model.encode([user_answer, correct_answer])\n",
        "            cosine_similarity = util.cos_sim(embeddings[0], embeddings[1])\n",
        "            return cosine_similarity.item()\n",
        "\n",
        "        def named_entity_adjustment(user_answer, correct_answer, base_similarity):\n",
        "            doc_user = nlp(user_answer)\n",
        "            doc_correct = nlp(correct_answer)\n",
        "            user_entities = [(ent.text, ent.label_) for ent in doc_user.ents]\n",
        "            correct_entities = [(ent.text, ent.label_) for ent in doc_correct.ents]\n",
        "\n",
        "            if user_entities and correct_entities:\n",
        "                if user_entities == correct_entities:\n",
        "                    base_similarity += 0.1\n",
        "                elif len(user_entities) == len(correct_entities):\n",
        "                    base_similarity += 0.05\n",
        "\n",
        "            return min(base_similarity, 1.0)\n",
        "\n",
        "        def tfidf_similarity(user_answer, correct_answer):\n",
        "            if not user_answer.strip() or not correct_answer.strip():\n",
        "                return 0\n",
        "\n",
        "            vectorizer = TfidfVectorizer()\n",
        "            try:\n",
        "                vectors = vectorizer.fit_transform([user_answer, correct_answer])\n",
        "                tfidf_sim = cs_tfidf(vectors[0], vectors[1])[0][0]\n",
        "            except ValueError:\n",
        "                tfidf_sim = 0\n",
        "            return tfidf_sim\n",
        "\n",
        "        def emotion_adjustment(user_answer, correct_answer, base_similarity):\n",
        "            user_emotion = emotion_classifier(user_answer)[0]['label']\n",
        "            correct_emotion = emotion_classifier(correct_answer)[0]['label']\n",
        "\n",
        "            if user_emotion == correct_emotion:\n",
        "                base_similarity += 0.1\n",
        "            else:\n",
        "                base_similarity -= 0.1\n",
        "\n",
        "            return min(base_similarity, 1.0)\n",
        "\n",
        "        def real_life_similarity(user_answer, correct_answer, base_similarity):\n",
        "            user_words = user_answer.split()\n",
        "            correct_words = correct_answer.split()\n",
        "\n",
        "            similar_word_count = 0\n",
        "            similarity_sum = 0\n",
        "\n",
        "            for user_word in user_words:\n",
        "                for correct_word in correct_words:\n",
        "                    user_embedding = sentence_model.encode(user_word)\n",
        "                    correct_embedding = sentence_model.encode(correct_word)\n",
        "                    similarity = util.cos_sim(user_embedding, correct_embedding).item()\n",
        "\n",
        "                    if similarity > 0.5:\n",
        "                        similarity_sum += similarity\n",
        "                        similar_word_count += 1\n",
        "\n",
        "            if similar_word_count > 0:\n",
        "                avg_similarity = similarity_sum / similar_word_count\n",
        "                base_similarity += avg_similarity * 0.1\n",
        "            else:\n",
        "                avg_similarity = 0\n",
        "\n",
        "            return min(base_similarity, 1.0), avg_similarity\n",
        "\n",
        "        def evaluate_and_store_score(user_answer, correct_answer, question, question_number):\n",
        "            similarity_score = semantic_similarity(user_answer, correct_answer)\n",
        "            tfidf_score = tfidf_similarity(user_answer, correct_answer)\n",
        "            similarity_score = named_entity_adjustment(user_answer, correct_answer, similarity_score)\n",
        "            similarity_score = emotion_adjustment(user_answer, correct_answer, similarity_score)\n",
        "\n",
        "            similarity_score, real_life_score = real_life_similarity(user_answer, correct_answer, similarity_score)\n",
        "\n",
        "            combined_similarity = (similarity_score + tfidf_score) / 2\n",
        "            combined_similarity = min(max(combined_similarity, 0), 1.0)\n",
        "\n",
        "            lst.append(combined_similarity)\n",
        "\n",
        "            if combined_similarity >= 0.80:\n",
        "                remark = \"The answer is correct.\"\n",
        "            elif combined_similarity >= 0.65:\n",
        "                remark = \"The answer is similar in some cases.\"\n",
        "            elif combined_similarity >= 0.40:\n",
        "                remark = \"The answer is similar in general terms.\"\n",
        "            else:\n",
        "                remark = \"The answer is wrong or not similar.\"\n",
        "\n",
        "            percentage = combined_similarity * 100\n",
        "\n",
        "            results[f'question{question_number}'] = {\n",
        "                'question': question,\n",
        "                'combined_score': combined_similarity,\n",
        "                'percentage': percentage,\n",
        "                'remark': remark,\n",
        "                'real_life_similarity': real_life_score\n",
        "            }\n",
        "\n",
        "        def plot_scores():\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            plt.plot(lst, marker='o', color='b')\n",
        "            plt.title('Combined Similarity Scores Over Time')\n",
        "            plt.xlabel('Attempt')\n",
        "            plt.ylabel('Similarity Score')\n",
        "            plt.grid(True)\n",
        "            file_path = 'similarity_scores_plot.jpg'\n",
        "            plt.savefig(file_path, format='jpg', bbox_inches='tight')\n",
        "            return file_path\n",
        "\n",
        "        # Evaluate answers\n",
        "        for i in range(1, questions_dict['nq'] + 1):\n",
        "            correct_answer = questions_dict.get(f'a{i}', '').strip()\n",
        "            user_answer = answers_dict.get(f'uan{i}', '').strip()\n",
        "\n",
        "            if correct_answer:\n",
        "                evaluate_and_store_score(user_answer, correct_answer, questions_dict.get(f'q{i}', ''), i)\n",
        "            else:\n",
        "                question_text = questions_dict.get(f'q{i}', '').strip()\n",
        "                evaluate_and_store_score(user_answer, question_text, question_text, i)\n",
        "\n",
        "        # Plot scores\n",
        "        image_path = plot_scores()\n",
        "\n",
        "        return results, image_path, lst\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "questions = {\n",
        "    'nq': 3,\n",
        "    'q1': 'What is the capital of France?',\n",
        "    'a1': 'Paris',\n",
        "    'q2': 'Who wrote Hamlet?',\n",
        "    'a2': 'Shakespeare',\n",
        "    'q3': 'What is 2 + 2?',\n",
        "    'a3': '4',\n",
        "}\n",
        "\n",
        "answers = {\n",
        "    'uan1': 'Paris',\n",
        "    'uan2': 'Shakespeare',\n",
        "    'uan3': '4'\n",
        "}\n",
        "\n",
        "results, image_path, scores = evaluate_answers(questions, answers)\n",
        "\n",
        "print(f\"\\nThe graph has been saved at: {image_path}\")\n",
        "print(\"\\nResults for each question:\")\n",
        "for key, value in results.items():\n",
        "    print(f\"{key}: {value}\")\n",
        "print(f\"\\nScores: {scores}\")"
      ],
      "metadata": {
        "id": "QwRt1VG7BzWi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}